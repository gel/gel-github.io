+++
title = "LLM Research"
weight = 2
sort_by = "weight"
insert_anchor_links = "right"
+++

## A Curated Collection of LLM Research Papers

Welcome to the repository of knowledge where the pursuit of understanding Large Language Models (LLMs) becomes a shared adventure. This chapter is dedicated to providing you with a meticulously curated list of research papers, each accompanied by a succinct summary highlighting the core insights. Along with direct links to the original works hosted on arXiv, this collection aims to serve as a gateway to the depths of LLM research and development.

Prepare to dive into the technical breakthroughs, innovative methodologies, and the latest findings within the realm of LLMs. Whether you are a seasoned researcher, an industry professional, or simply an AI enthusiast, this compilation is set to be an invaluable resource in your journey through the landscape of generative AI.

## Research Papers Summary and Insights

Below is the list of selected research papers that have significantly contributed to the field of Large Language Models. Each entry includes a brief summary and an arXiv link to the full paper for a comprehensive read.

### LLM Pretraining / Fine-Tuning

- [Survey] Instruction Tuning for Large Language Models

-  [RA-DIT] Retrieval-Augmented Dual Instruction Tuning

- [Sequential Monte Carlo] Steering of LLMs using Probabilistic Programs

### LLM Agents

- [RetroFormer] Retrospective LL Agents with Policy Gradient Optimization

### LLM Optimization

- [LLM-in-a-Flash] Efficient LLM Inference with Limited Memory

- [RoPE] RoFormer: Enhanced Transformer with Rotary Position Embedding

- [LORA] LOw-RAnk Adaptation of LLM

- [Speculative] Fast Inference from Transformers via Speculative Decoding


- [GQA] Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints


- [Multi-Heads Sharing] Fast Transformer Decoding: One Write-Head is All You Need

- [MoE] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts Layer


- [MoE] Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for LLM

### LLM Prompting

- [MedPrompt] Can Generalist Foundation Models Outcompete Special-Purpose Tuning? 

- [URIAL] The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning

- [CoVE] Chain-of-Verification Reduces Hallucinations in LLM Models

### LLM Benchmarks & Evaluation

- [Benchmark] Generating Benchmarks for Factuality Evaluation of Language Models

### LLM Multi-Modal / Vision

- [Point-E] A System for Generating 3D Point Clouds from Complex Prompts

- [CLIP] Connecting text and images

### LLM Models

- [Gemini] A Family of Highly Capable Multimodal Models

---

As we embark on this scholarly expedition, remember that this is just the beginning. The field of LLMs is ever-evolving, with new discoveries and insights emerging regularly. Keep this page bookmarked, and revisit often to stay updated with the latest research that shapes the future of generative AI and LLMs.
